{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical evaluation methods for multiobjective reinforcement learning algorithms\n",
    "Peter Vamplew, Richard Dazeley, Adam Berry, Rustam Issabekov, Evan Dekker\n",
    "2010\n",
    "\n",
    "### Issues\n",
    "- small test set\n",
    "- inconsistency in algorithm performance representation\n",
    "\n",
    "\n",
    ">__hypervolume__ : the area between reference point(R) or pareto front(P) and the solution(between in terms with dominance)\n",
    "\n",
    "## 5.Single policy\n",
    "### single objective RL evaluation\n",
    ">* how closely policy matches the optimal policy\n",
    ">* how quickly they converge to this final policy.\n",
    "\n",
    "## 5.2. Multi-objective accumulated reward metrics\n",
    "- use accumulated reward instead of reward function\n",
    "- if two sets are incomparable, use hypervolume metric\n",
    "- favored set would depend on the metric\n",
    "\n",
    "## 5.3. multi-objective regret\n",
    ">$R_T = T_{\\rho^*} - \\underset {t=0} {\\overset {T-1} {\\sum}} r_t$  \n",
    "wherer $\\rho^*$ is the average vector reward when following an optimal policy,  \n",
    "$r_t$ is the vector reward recieved at time t,  \n",
    "T is the number of time steps over which the regret is measured\n",
    "\n",
    "instead, use  \n",
    "> $R_s = \\sqrt{\\overset {N} {\\underset {j=0} {\\sum}}max(0, R_{T,j})^2}$\n",
    "\n",
    "issue here is choosing appropriate Pareto front to be used as target policy  \n",
    "-> how easily we can determine the parameter can affect\n",
    "\n",
    "\n",
    "## 5.5.1 General algorithm for simulated user testing\n",
    "1. Iterate through a range of preference parameter settings. For each set of parameter values:\n",
    "    1. Map the parameter settings to a point $P_T$ which is a member of the Pareto front\n",
    "    2. Use the policy corresponding to $P_T$ as the target policy.\n",
    "    3. Calculate the regret vector $R_T$\n",
    "    4. Calculate $R_S$ via (2).\n",
    "\n",
    "2. Summarise the $R_S$ values across the range of parameter settings.\n",
    "\n",
    "## 5.5.2 Simulated user testing of scalarised Q-learning\n",
    "1. Establish the maximum and minimum bounds achievable for each objective by Pareto optimal strategies, either from the true front, or from an approximate front estimated from the results of previous experiments.\n",
    "2. Construct a hyperplane passing through the extremal points of the front (it is reasonable to assume flat).\n",
    "3. For a given set of preference weights W, calculate a point $P_H$ on the hyperplane by treating W as a set of weights for a barycentric coordinate system with basis points being the extrema of the hyperplane.\n",
    "4. Identify the point $P_T$ on the Pareto front which minimises $|P_H âˆ’ P_T|$.\n",
    "\n",
    "## 7.4 Summary of demonstration\n",
    "* The offline hypervolume metric has been shown to clearly indicate differences between the quality of the final Pareto fronts found by MORL algorithms.\n",
    "* for online performance, regret measure is better.\n",
    "* where regret cannot be calculated as the true front is not known, the online hypervolume still provides useful information\n",
    "\n",
    "## Conclusion\n",
    "* online learning of single policies and offline learning of multiple policies require alternative experimental methods and evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
